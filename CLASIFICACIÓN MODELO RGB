//MODELO DE CLASIFICACION RGB

//INTALAMOS LO NECESARIO
pip install opencv-python

//IMPORTAMOS LIBRERIAS
import numpy as np
import os
import re
import matplotlib.pyplot as plt
%matplotlib inline
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import keras
from keras.utils import to_categorical
from keras.models import Sequential,Model
from tensorflow.keras.layers import Input
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.layers import BatchNormalization
from keras.layers import ELU, PReLU, LeakyReLU
import albumentations as A

//CARGAR IMAGENES
dirname = os.path.join(os.getcwd(), 'ISITFG')
imgpath = dirname + os.sep 
 
images = []
directories = []
dircount = []
prevRoot=''
cant=0
 
print("leyendo imagenes de ",imgpath)
 
for root, dirnames, filenames in os.walk(imgpath):
    for filename in filenames:
        if re.search("\.(jpg|jpeg|png|bmp|tiff)$", filename):
            cant=cant+1
            filepath = os.path.join(root, filename)
            image = plt.imread(filepath)
            images.append(image)
            b = "Leyendo..." + str(cant)
            print (b, end="\r")
            if prevRoot !=root:
                print(root, cant)
                prevRoot=root
                directories.append(root)
                dircount.append(cant)
                cant=0
dircount.append(cant)
 
dircount = dircount[1:]
dircount[0]=dircount[0]+1
print('Directorios leidos:',len(directories))
print("Imagenes en cada directorio", dircount)
print('suma Total de imagenes en subdirs:',sum(dircount))

//CREAMOS ETIQUETAS Y CLASES
labels=[]
indice=0
for cantidad in dircount:
    for i in range(cantidad):
        labels.append(indice)
    indice=indice+1
print("Cantidad etiquetas creadas: ",len(labels))
 
deportes=[]
indice=0
for directorio in directories:
    name = directorio.split(os.sep)
    print(indice , name[len(name)-1])
    deportes.append(name[len(name)-1])
    indice=indice+1
 
y = np.array(labels)
X = np.array(images, dtype=np.uint8) 
 
classes = np.unique(y)
nClasses = len(classes)
print('Total number of outputs : ', nClasses)
print('Output classes : ', classes)

//CREAMOS SETS DE ENTRENAMIENTO Y TEST
train_X,test_X,train_Y,test_Y = train_test_split(X,y,test_size=0.2)
print('Training data shape : ', train_X.shape, train_Y.shape)
print('Testing data shape : ', test_X.shape, test_Y.shape)
 
train_X = train_X.astype('float32')
test_X = test_X.astype('float32')
train_X = train_X / 255.
test_X = test_X / 255.
 
# Change the labels from categorical to one-hot encoding
train_Y_one_hot = to_categorical(train_Y)
test_Y_one_hot = to_categorical(test_Y)
 
# Display the change for category label using one-hot encoding
print('Original label:', train_Y[0])
print('After conversion to one-hot:', train_Y_one_hot[0])
 
train_X,valid_X,train_label,valid_label = train_test_split(train_X, train_Y_one_hot, test_size=0.2, random_state=13)
 
print(train_X.shape,valid_X.shape,train_label.shape,valid_label.shape)

//CREAMOS LAS TRANSFORMACIONES QUE HEMOS QUERIDO DARLE A LAS IMAGENES Y ASI UTILIZAR LALIBRERIA ALBUMENTATIONS
//ES OPCIONAL POR SI QUIERE PROBAR EL ENTRENAMIENTO DE DIFERENTE FORMA
transform = A.Compose([
    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),
    A.Rotate(limit=15, p=0.5),
    A.Blur(blur_limit=3, p=0.5),
    A.HorizontalFlip(p=0.5)
])

transformed_images = []
for image in train_X:
    transformed = transform(image=image)
    transformed_image = transformed["image"]
    transformed_images.append(transformed_image)

train_X = np.array(transformed_images)

train_X,valid_X,train_label,valid_label = train_test_split(train_X, train_Y_one_hot, test_size=0.2, random_state=13)
 
print(train_X.shape,valid_X.shape,train_label.shape,valid_label.shape)

//CREAMOS LA RED

//CREAMOS LA RED
INIT_LR = 1e-3
epochs = 6
batch_size = 64
 
Isi_model = Sequential()
Isi_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',padding='same',input_shape=(300,400,3)))
Isi_model.add(LeakyReLU(alpha=0.1))
Isi_model.add(MaxPooling2D((2, 2),padding='same'))
Isi_model.add(Dropout(0.5))
 
Isi_model.add(Flatten())
Isi_model.add(Dense(64, activation='linear'))
Isi_model.add(LeakyReLU(alpha=0.1))
Isi_model.add(Dropout(0.5)) 
Isi_model.add(Dense(nClasses, activation='softmax'))

Isi_model.summary()
 
Isi_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adagrad(lr=INIT_LR, decay=INIT_LR / 100),metrics=['accuracy'])

//COMPILAMOS LA RED Y LA GUARDAMOS PARA NO TENER QUE HACER EL ENTRENAMIENTO Y AHORRARNOS ALGO DE TIEMPO
Isi_train = Isi_model.fit(train_X, train_label, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_X, valid_label))
 
Isi_model.save("Isi_NORMAL_MAS.h5py")

//VEMOS LOS RESUTADOS
test_eval = Isi_model.evaluate(test_X, test_Y_one_hot, verbose=1)

print('Test loss:', test_eval[0])
print('Test accuracy:', test_eval[1])

//IMPRIME LAS GRAFICAS DE LOS RESULTADOS
accuracy = Isi_train.history['accuracy']
val_accuracy = Isi_train.history['val_accuracy']
loss = Isi_train.history['loss']
val_loss = Isi_train.history['val_loss']
epochs = range(len(accuracy))
plt.plot(epochs, accuracy, 'bo', label='Training accuracy')
plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()

